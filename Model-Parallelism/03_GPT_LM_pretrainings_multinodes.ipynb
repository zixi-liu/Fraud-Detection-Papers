{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Multi-Node Distributed Training Strategies\n",
    "\n",
    "In this notebook, we will learn how to run [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) GPT pretraing on multiple nodes.\n",
    "\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Run simple multi-node training of Megatron-LM scripts\n",
    "* Run a hybrid multi-node execution with data, tensor and pipeline parallel distributions\n",
    "\n",
    "\n",
    "**[3.1 Multi-Node Training Execution of Megatron-LM GPT Pretraining](#1.1-The-hardware-overview)<br>**\n",
    "**[3.2 Multi-Node Execution with Data Parallelism](#1.1-The-hardware-overview)<br>**\n",
    "**[3.3 Inter/Intra Node Communications](#1.1-The-hardware-overview)<br>**\n",
    "**[3.4 Profiling](#1.1-Profiling)<br>**\n",
    "**[3.5 Exercise: Hybrid Distributed Training Strategy](#1.1-The-hardware-overview)<br>**\n",
    "**[3.6 Increase The Batchsize / GPU](#1.1-The-hardware-overview)<br>**\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Multi-Node Training Execution of Megatron-LM GPT Pretraining\n",
    "\n",
    "In the previous notebook, we submitted our jobs in an interactive session after allocating 1 node. \n",
    "\n",
    "For multi-node jobs, we need to rely on the SLURM scheduler. By default, multi-node training with Megatron-LM uses the [NVIDIA Collective Communications Library - NCCL](https://developer.nvidia.com/nccl) distributed backend of the [PyTorch distributed launcher](https://pytorch.org/docs/stable/distributed.html).\n",
    "\n",
    "For 2-Node execution, we will use `SBATCH` scripting. While python execution commands and their arguments remain similar to how it was done for Single Node, we will need additional `SBATCH` arguments for resource allocation. \n",
    "\n",
    "Let's start by executing Megatron-LM GPT pretraining on 2 nodes using only Data Parallelism, meaning that the model is copied on the 4 allocated GPUs, each processing different data batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Multi-Node Execution with Data Parallelism\n",
    "\n",
    "In the previous 2-GPU data parallelism execution, the batch size processed by each GPU was 2 (set by `--micro-batch-size`) corresponding to a global batch size of 4 (set by `--global-batch-size`). \n",
    "\n",
    "When using 4 GPUs, we can keep the micro batch size per GPUs to 2 and set the global batch size to 8 (MICRO_BATCH_SIZE $ \\times $ 4).\n",
    "\n",
    "Let's have a look at the script before allocating resources and executing it. \n",
    "\n",
    "Notice the `SBATCH` arguments for allocating resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=dli_2nodes\n",
      "#SBATCH --nodes=2\n",
      "#SBATCH --ntasks-per-node=1       \n",
      "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
      "#SBATCH -o /dli/megatron/logs/%j.out\n",
      "#SBATCH -e /dli/megatron/logs/%j.err\n",
      "\n",
      "set -x -e\n",
      "\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "\n",
      "# Distributed training args\n",
      "NNODES=2\n",
      "GPUS_PER_NODE=2\n",
      "TP_SIZE=1\n",
      "PP_SIZE=1 \n",
      "\n",
      "# SLURM args\n",
      "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
      "MASTER_PORT=6000\n",
      "\n",
      "# Distributed training \n",
      "MICRO_BATCH_SIZE=2\n",
      "GLOBAL_BATCH_SIZE=8    # <--- CHANGED HERE\n",
      "\n",
      "# Model architecture \n",
      "NLAYERS=12\n",
      "NHIDDEN=768\n",
      "NHEADS=32\n",
      "SEQ_LEN=1024\n",
      "VOCAB_SIZE=50257\n",
      "\n",
      "# Data Paths\n",
      "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
      "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
      "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
      "LOGS_PATH=/dli/megatron/logs\n",
      "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
      "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
      "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
      "\n",
      "NAME=\"log_2Nodes4GPUS\"       # <--- CHANGED HERE \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OPTIMIZER_ARGS=\" \\\n",
      "            --optimizer adam \\\n",
      "            --adam-beta1 0.9 \\\n",
      "            --adam-beta2 0.95 \\\n",
      "            --adam-eps 1e-8 \\\n",
      "            --lr 6e-5 \\\n",
      "            --min-lr 6e-6 \\\n",
      "            --lr-decay-style cosine \\\n",
      "            --lr-decay-iters 800 \\\n",
      "            --lr-warmup-fraction .01 \\\n",
      "            --clip-grad 1.0 \\\n",
      "            --weight-decay 1e-1 \\\n",
      "            --exit-duration-in-mins 1190 \\\n",
      "              \"\n",
      "\n",
      "GPT_ARGS=\" \\\n",
      "            --num-layers $NLAYERS \\\n",
      "            --hidden-size $NHIDDEN \\\n",
      "            --num-attention-heads $NHEADS \\\n",
      "            --seq-length $SEQ_LEN \\\n",
      "            --max-position-embeddings $SEQ_LEN \\\n",
      "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
      "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
      "            --train-iters 100 \\\n",
      "            --vocab-file $VOCAB_FILE \\\n",
      "            --merge-file $MERGE_FILE \\\n",
      "            --init-method-std 0.006 \\\n",
      "            $OPTIMIZER_ARGS \\\n",
      "        \"\n",
      "\n",
      "OUTPUT_ARGS=\" \\\n",
      "            --log-interval 10 \\\n",
      "            --save-interval 300 \\\n",
      "            --eval-interval 1000 \\\n",
      "            --eval-iters 10 \\\n",
      "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
      "            --tensorboard-queue-size 1 \\\n",
      "            --log-timers-to-tensorboard \\\n",
      "            --log-batch-size-to-tensorboard \\\n",
      "            --log-validation-ppl-to-tensorboard \\\n",
      "            --profile-execution True \\\n",
      "            --profile-name baseline \\\n",
      "            \"\n",
      "\n",
      "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
      "             --nproc_per_node $GPUS_PER_NODE \\\n",
      "             --nnodes $NNODES \\\n",
      "             --master_addr $MASTER_ADDR \\\n",
      "             --master_port $MASTER_PORT \\\n",
      "             \"\n",
      "\n",
      "export CMD=\" \\\n",
      "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
      "             --tensor-model-parallel-size $TP_SIZE \\\n",
      "             --pipeline-model-parallel-size $PP_SIZE \\\n",
      "             $GPT_ARGS \\\n",
      "             $OUTPUT_ARGS \\\n",
      "             --save $CHECKPOINT_PATH \\\n",
      "             --data-path $DATA_PATH \\\n",
      "             --data-impl mmap \\\n",
      "             --split 949,50,1 \\\n",
      "             --distributed-backend nccl \\\n",
      "           \"\n",
      "\n",
      "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# have a look at Megaton-LM GPT pretraining execution on 2 nodes\n",
    "! cat /dli/code/pretrain_gpt_2Node4GPU.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the sbatch [script pretrain_gpt_2Node4GPU.sh](./code/pretrain_gpt_2Node4GPU.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 10\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                10  slurmpar dli_2nod    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# submit the 2 nodes jobs\n",
    "! sbatch /dli/code/pretrain_gpt_2Node4GPU.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the GPU usage by running the `nvidia-smi` command on the master lab node. After a few seconds, when the nodes are allocated and the script begins its execution, we should see the GPUs 0,1,2,3 utilized, as shown below. Please notice that if you are running Megatron-LM for the first time in the class, the code will need about 6 minutes to be compiled. Until there, you will not be able to see any GPU activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/2N_4gpus_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 17:40:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Graphics Device     On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   100W / 300W |  12482MiB / 81251MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Graphics Device     On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P0   100W / 300W |  12482MiB / 81251MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Graphics Device     On   | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   103W / 300W |  12482MiB / 81251MiB |     98%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Graphics Device     On   | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   323W / 300W |  12482MiB / 81251MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU utilization on the master node\n",
    "! sleep 10\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the Megatron GPT-3 pretraining, we can check the generated [logs](./megatron/logs/log_2Nodes4GPUS.txt) during execution.\n",
    "\n",
    "Let's first verify the world size of our run. We should see this:\n",
    "```\n",
    "using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n"
     ]
    }
   ],
   "source": [
    "! grep \"using world size:\" /dli/megatron/logs/log_2Nodes4GPUS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the performance of the GPT pretraining on 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       10/     100 | consumed samples:           80 | elapsed time per iteration (ms): 457.9 | learning rate: 6.000E-05 | global batch size:     8 | lm loss: 1.048261E+01 | loss scale: 1.0 | grad norm: 3.804 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 1907.2294921875 | max allocated: 10582.0205078125 | reserved: 10922.0 | max reserved: 10922.0\n",
      " iteration       20/     100 | consumed samples:          160 | elapsed time per iteration (ms): 297.8 | learning rate: 5.997E-05 | global batch size:     8 | lm loss: 9.940797E+00 | loss scale: 1.0 | grad norm: 2.449 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       30/     100 | consumed samples:          240 | elapsed time per iteration (ms): 298.3 | learning rate: 5.990E-05 | global batch size:     8 | lm loss: 9.499748E+00 | loss scale: 1.0 | grad norm: 2.510 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       40/     100 | consumed samples:          320 | elapsed time per iteration (ms): 314.3 | learning rate: 5.978E-05 | global batch size:     8 | lm loss: 9.071992E+00 | loss scale: 1.0 | grad norm: 2.382 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       50/     100 | consumed samples:          400 | elapsed time per iteration (ms): 313.0 | learning rate: 5.963E-05 | global batch size:     8 | lm loss: 8.672728E+00 | loss scale: 1.0 | grad norm: 2.242 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       60/     100 | consumed samples:          480 | elapsed time per iteration (ms): 426.9 | learning rate: 5.943E-05 | global batch size:     8 | lm loss: 8.312651E+00 | loss scale: 1.0 | grad norm: 2.270 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       70/     100 | consumed samples:          560 | elapsed time per iteration (ms): 310.0 | learning rate: 5.919E-05 | global batch size:     8 | lm loss: 8.117979E+00 | loss scale: 1.0 | grad norm: 2.113 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       80/     100 | consumed samples:          640 | elapsed time per iteration (ms): 311.5 | learning rate: 5.891E-05 | global batch size:     8 | lm loss: 7.795219E+00 | loss scale: 1.0 | grad norm: 2.016 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       90/     100 | consumed samples:          720 | elapsed time per iteration (ms): 307.2 | learning rate: 5.858E-05 | global batch size:     8 | lm loss: 7.669200E+00 | loss scale: 1.0 | grad norm: 1.599 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration      100/     100 | consumed samples:          800 | elapsed time per iteration (ms): 312.6 | learning rate: 5.822E-05 | global batch size:     8 | lm loss: 7.453542E+00 | loss scale: 1.0 | grad norm: 1.295 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "saving checkpoint at iteration     100 to /dli/megatron/checkpoints\n",
      "  successfully saved checkpoint at iteration     100 to /dli/megatron/checkpoints\n"
     ]
    }
   ],
   "source": [
    "! grep iteration /dli/megatron/logs/log_2Nodes4GPUS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract logs, we can see the training performance while using 4 GPUs (2 GPUs per node). \n",
    "\n",
    "```\n",
    "iteration      100/     100 | consumed samples:          800 | elapsed time per iteration (ms): 537.3 | learning rate: 5.822E-05 | global batch size:     8 | lm loss: 7.448950E+00 | loss scale: 1.0 | grad norm: 1.303 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
    "```\n",
    "Compare this with 1 Node executions, and discuss it with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Inter/Intra Node Communications \n",
    "\n",
    "In the previous run, you may have noticed that we added the NCCL variable `NCCL_DEBUG=INFO` in order to output the NCCL debug log traces during training as shown bellow. \n",
    "\n",
    "![title](images/nodes_communication.png)\n",
    "\n",
    "\n",
    "This will allow us to check the type of networking used between GPUs and nodes during the training. \n",
    "\n",
    "The direct GPU-to-GPU communication are reposted as `P2P/IPC` while internode communications are reported to be through the `NET/Socket/0`.\n",
    "In our configuration, direct GPU-to-GPU communication should be used within nodes: GPU0<->GPU1 and GPU2<->GPU3). \n",
    "\n",
    "Let's check what NCCL reported in the generated [logs](./megatron/logs/log_2Nodes4GPUS.txt) during the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 00 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 01 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2182:2349 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:2182:2349 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2182:2349 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:2182:2349 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 00 : 2[300000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 01 : 2[300000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 00 : 0[100000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2348 [0] NCCL INFO Channel 01 : 0[100000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 00 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2182:2373 [1] NCCL INFO Channel 00 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 01 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 00 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:2182:2373 [1] NCCL INFO Channel 01 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 01 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 00 : 2[300000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2182:2373 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:2182:2373 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 01 : 2[300000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 00 : 0[100000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2181:2372 [0] NCCL INFO Channel 01 : 0[100000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2182:2379 [1] NCCL INFO Channel 31/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2181:2381 [0] NCCL INFO Channel 31/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2181:2387 [0] NCCL INFO Channel 31/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2182:2390 [1] NCCL INFO Channel 31/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2182:3024 [1] NCCL INFO Channel 31/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 00/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 01/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 02/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 03/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 04/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 05/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 06/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 07/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 08/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 09/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 10/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 11/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 12/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 13/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 14/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 15/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 16/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 17/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 18/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 19/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 20/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 21/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 22/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 23/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 24/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 25/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 26/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 27/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 28/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 29/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 30/32 :    0\n",
      "slurmnode1:2181:3025 [0] NCCL INFO Channel 31/32 :    0\n"
     ]
    }
   ],
   "source": [
    "! grep Channel /dli/megatron/logs/log_2Nodes4GPUS.txt | grep slurmnode1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 Monitoring and Profiling the Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, monitoring the training runs with Megatron-LM was done via the text log files. However, monitoring training is also possible through tensorboard visualization of the hyper parameters and training/evaluation metrics. In addition, visualizing can be helpful for debugging and optimizing the models during training.\n",
    "\n",
    "## 3.4.1 Training Metrics Visualization on Tensorboard\n",
    "\n",
    "<img src=\"images/tensorboard1.png\" width=\"950\"/>\n",
    "\n",
    "In the previous Megatron-LM runs, we set the arguments for recording Tensorboard events. The graphs of all the previous experiments are available in the folder `megatron/tensorboard/`.\n",
    "\n",
    "Execute the next cell to create a link to Tensorboard for the browser. Then, click the link to see graphs of experiment metrics saved in the specified `Tensorboard` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Pytorch Profiler with Tensorboard\n",
    "\n",
    "Several existing profiling tools can be used to investigate bottlenecks during the training and inference processes. This allows us to identify the most expensive operations, issues such as GPU starvation, or unnecessary operations. \n",
    "\n",
    "In this class, we will use the [Pytorch Profiler](https://pytorch.org/docs/stable/profiler.html), a tool collecting performance metrics during Pytorch executions. The [Tensorboard-plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) with the PyTorch Profiler provides a visualization of each GPU process, with several measures of operations running on each GPU, whether using hardware accelerator (TensorCores). It also provides recommendations to improve the process.\n",
    "\n",
    "We will trace only the execution of the GPU_0, monitoring operations during 2 training steps. We specified the profiling by setting `profile-execution` to *True* and providing the `profile-name` to *baseline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the previous run, profiling is available on the Tensorboard link at the `pytorch_profiler` tab.\n",
    "<img src=\"images/profiling1.png\" width=\"900\"/>\n",
    "\n",
    "In case you already closed the Tensorboard page, you can re-generate the link by executing the next cell. Click the link to open Tensorboard and then, go to the `PYTORCH_PROFILER` tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiler homepage shows an overview of tracing. \n",
    "\n",
    "\n",
    "### The Profiling Overview\n",
    "Several panels are shown on the homepage:\n",
    "- The GPU Summary: shows the GPU configuration and utilization. In our previous run, no TensorCores are used as we have not enabled Mixed Precision training (will see this in the next notebook). \n",
    "\n",
    "- The Step Time Breakdown: shows the execution time spent in each operation. In our previous run,  48.5% of the step time is in communication and 41.7% in Kernel execution time on GPU \n",
    "\n",
    "- Performance Recommendation: Providing recommendations on how to improve the process. There are a few recommendations for our previous run, such as reducing communication cost by using Gradient Accumulation or increasing the batch size. The profiler suggests also enabling Automatic Mixed Precision to speedup Kernels. In the next section, let us first start by increasing the batch size and see its impact in the process (speedup and memory consumption).\n",
    "\n",
    "### The Memory View\n",
    "Let's have a look at the Memory View showing the memory allocation and deallocation of our run. We can zoom into the memory curve to see the related memory events.\n",
    "\n",
    "<img src=\"images/profiling4.png\" width=\"750\"/>\n",
    "\n",
    "The memory allocation followed by deallocation corresponds to the forward and backward pass. This process is traced twice as we traced 2 training steps. We can also see that a peak of ~10GB of the GPU device memory is used during the training step. We can increase the memory usage by increasing the model or the batch size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\\\n",
    "Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints\n",
    "! rm /dli/megatron/checkpoints/* -r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.5 Increase The Batchsize / GPU\n",
    "\n",
    "**Special Warning:** In this section, Out Of Memory (OOM) issues are expected! No problem, we will see a few solutions addressing this problem in the next notebook.\n",
    "\n",
    "\n",
    "The size of the current GPT model fits into the GPU memory and thus does not require model distribution (tensor or pipeline parallelism). \\\n",
    "Using only data parallelism, we can improve the training throughput (number of sequences processed per second) by increasing the batch size processed per GPU until reaching the maximum batch size that fits on GPU memory.\\\n",
    "Let's check this by doubling the data processed by each GPU. When using 4 GPUs with only data parallelism (DP_SIZE $= 4$), by increasing the micro batch size per GPUs from 2 to 4, the global batch size should be MICRO_BATCH_SIZE $\\times$ TP_SIZE $= 16$.\n",
    "\n",
    "Let's prepare the training script for this scenario by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=32      # <--- CHANGED HERE\n",
    "GLOBAL_BATCH_SIZE=128    # <--- CHANGED HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_DP_4_MBS_4\"\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name DP_4_MBS_4 \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh](./code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh) and check the SLURM queue using the squeue command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 12\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                12  slurmpar dli_2nod    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# submit the 2 nodes jobs\n",
    "! sbatch /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_4.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the Megatron GPT-3 pretraining performance by looking at the generated [logs](./megatron/logs/log_2Nodes4GPUS_hybrid_solution.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0] (after 10 iterations) memory (MB) | allocated: 1905.9794921875 | max allocated: 19084.1767578125 | reserved: 19876.0 | max reserved: 19876.0\n",
      " iteration       10/     100 | consumed samples:          160 | elapsed time per iteration (ms): 549.2 | learning rate: 6.000E-05 | global batch size:    16 | lm loss: 1.045963E+01 | loss scale: 1.0 | grad norm: 2.574 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       20/     100 | consumed samples:          320 | elapsed time per iteration (ms): 407.1 | learning rate: 5.997E-05 | global batch size:    16 | lm loss: 9.929971E+00 | loss scale: 1.0 | grad norm: 2.603 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       30/     100 | consumed samples:          480 | elapsed time per iteration (ms): 405.8 | learning rate: 5.990E-05 | global batch size:    16 | lm loss: 9.441203E+00 | loss scale: 1.0 | grad norm: 2.433 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       40/     100 | consumed samples:          640 | elapsed time per iteration (ms): 404.1 | learning rate: 5.978E-05 | global batch size:    16 | lm loss: 9.024923E+00 | loss scale: 1.0 | grad norm: 2.522 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       50/     100 | consumed samples:          800 | elapsed time per iteration (ms): 405.1 | learning rate: 5.963E-05 | global batch size:    16 | lm loss: 8.645444E+00 | loss scale: 1.0 | grad norm: 2.307 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       60/     100 | consumed samples:          960 | elapsed time per iteration (ms): 562.1 | learning rate: 5.943E-05 | global batch size:    16 | lm loss: 8.298282E+00 | loss scale: 1.0 | grad norm: 2.289 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       70/     100 | consumed samples:         1120 | elapsed time per iteration (ms): 404.4 | learning rate: 5.919E-05 | global batch size:    16 | lm loss: 8.054238E+00 | loss scale: 1.0 | grad norm: 2.224 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       80/     100 | consumed samples:         1280 | elapsed time per iteration (ms): 407.1 | learning rate: 5.891E-05 | global batch size:    16 | lm loss: 7.835020E+00 | loss scale: 1.0 | grad norm: 1.834 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration       90/     100 | consumed samples:         1440 | elapsed time per iteration (ms): 404.0 | learning rate: 5.858E-05 | global batch size:    16 | lm loss: 7.631736E+00 | loss scale: 1.0 | grad norm: 1.803 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      " iteration      100/     100 | consumed samples:         1600 | elapsed time per iteration (ms): 403.9 | learning rate: 5.822E-05 | global batch size:    16 | lm loss: 7.408549E+00 | loss scale: 1.0 | grad norm: 1.362 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "saving checkpoint at iteration     100 to /dli/megatron/checkpoints\n",
      "  successfully saved checkpoint at iteration     100 to /dli/megatron/checkpoints\n"
     ]
    }
   ],
   "source": [
    "! grep iteration /dli/megatron/logs/log_2Nodes4GPUS_DP_4_MBS_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No elapsed time per iteration is shown. **What just happened?!** \n",
    "\n",
    "Are we saturating the GPU memory? Run the cell bellow to search for RuntimeError in our logs. You should see:\n",
    "```\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 13.90 GiB already allocated; 302.00 MiB free; 14.43 GiB reserved in total by PyTorch)\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 1; 15.78 GiB total capacity; 13.90 GiB already allocated; 302.00 MiB free; 14.43 GiB reserved in total by PyTorch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 1; 79.35 GiB total capacity; 76.80 GiB already allocated; 915.19 MiB free; 76.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 1; 79.35 GiB total capacity; 76.80 GiB already allocated; 915.19 MiB free; 76.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 79.35 GiB total capacity; 76.80 GiB already allocated; 915.19 MiB free; 76.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 79.35 GiB total capacity; 76.80 GiB already allocated; 915.19 MiB free; 76.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "! grep \"RuntimeError\" /dli/megatron/logs/log_2Nodes4GPUS_DP_4_MBS_4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we should see `CUDA out of memory` errors which means that the GPU memory cannot handle training this transformer model size with the specified arguments.\n",
    "\n",
    "Doubling the amount of data processed per GPU is too much for the 16G of memory.  \n",
    "\n",
    "In the next lab, we will focus on how to address this issue by reducing the model's memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints and logs directory\n",
    "! rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---\n",
    "# 3.6 Exercise: Hybrid Distributed Training Strategy \n",
    "\n",
    "\n",
    "Let's configure a new Megatron-LM GPT pretraining execution on 2 nodes (4 GPUs) with both tensor and pipeline parallel by modifying the \"FIXME\" in the following cell. \n",
    "\n",
    "To use tensor and pipeline parallel, we can keep distributed the model using in 2 dimensions: 2 GPUs for tensor parallelism and 2 GPUs for pipeline parallelism. Thus, no more resources will remain for the data parallel. In this case, the `GLOBAL_BATCH_SIZE` should be downgraded to the same size as the `MICRO_BATCH_SIZE`.\n",
    "\n",
    "If you get stuck, view the [solution](solutions/ex3.4.ipynb) for a hint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=2        # <--- CHANGE HERE\n",
    "PP_SIZE=2         # <--- CHANGE HERE\n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=4         # <--- CHANGE HERE\n",
    "GLOBAL_BATCH_SIZE=4        # <--- CHANGE HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_hybrid\"\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name DP_PP \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_hybrid.sh](/dli/code/pretrain_gpt_2Node4GPU_hybrid.sh) for hybrid multi-node run and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "! sbatch /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand the performance of the Megatron GPT-3 pretraining, we can check the generated logs during the execution.\n",
    "\n",
    "Let's first look at the generated [logs](/dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt) and check world size of our executed hybrid run. We should see this:\n",
    "\n",
    "```\n",
    "using world size: 4, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep \"using world size:\" /dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training performance and GPU0 memory allocation and deallocation of our run. You should see a graph similar to the following: \n",
    "<img src=\"images/profiling_hybrid.png\" width=\"950\"/>\n",
    "\n",
    "What is the constant time step between the forward and backward pass? Discuss with the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep iteration /dli/megatron/logs/log_2Nodes4GPUS_hybrid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "! rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will see how to optimize the GPT pretraining using techniques such as Mixed Precision, Gradient Accumulation and Activation Checkpointing. Move on to [004_GPT_LM_pretrainings_optimizations.ipynb](04_GPT_LM_pretrainings_optimizations.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
