{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distributed Training Optimization\n",
    "\n",
    "In this notebook, we will learn how to quantify the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) GPT pretraining performance and see optimization techniques such as Mixed Precision, Gradient Accumulation and Activation Checkpointing.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Optimize multi-node training of Megatron-LM scripts using Automatic Mixed Precision and Activation Checkpointing \n",
    "* Understand how to compute training performance\n",
    "\n",
    "\n",
    "**[4.1 Mixed Precision Training](#1.1-The-hardware-overview)<br>**\n",
    "**[4.2 Activation Checkpoiting ](#1.1-The-hardware-overview)<br>**\n",
    "**[4.3 Gradient Accumulation](#1.1-The-hardware-overview)<br>**\n",
    "**[4.4 The Training Performance](#4.4-Compute-The-Training-Performance)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.1 Compute the Number of Parameters](#1.1.1-Check-The-Available-CPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.2 Exercise: Compute the Number of Parameters For Our Model](#1.1.2-Check-The-Available-GPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.3 Compute the Theoretical Peak FLOP per second per GPU](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.4 Estimate the Training Duration / Epoch](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs with the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.1 Mixed Precision Training \n",
    "\n",
    "<img src=\"images/AMP.png\" width=\"700\"/>\n",
    "\n",
    "**Automatic Mixed Precision (AMP)** allows using different numerical precisions when running mathematical operations. It performs some operations in half-precision format reducing the memory required while keeping single-precision in critical parts of the network.\n",
    "\n",
    "Training with Automatic Mixed Precision takes advantage of the hardware acceleration provided by Tensor Cores available in NVIDIA GPUs from Volta architecture (Volta, Turing, Ampere and future architectures). Learn more about training with AMP, refer to the [Mixed precision training documentation](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, Mixed Precision training was not enabled for the baseline run. We can see the details in the GPU Kernel view. \n",
    "\n",
    "<img src=\"images/profiling3.png\" width=\"650\"/>\n",
    "\n",
    "According to the Performance Recommendation on the baseline run in the overview tab (shown also in the figure of the section 3.4.2 Pytorch Profiler with Tensorboard), Kernels with 27% of the total time are launched by Tensor Core eligible operators. We can speed up the operations by enabling Automatic Mixed Precision with FP16.\n",
    "\n",
    "To run Megatron-LM pretraining in Mixed Precision, simply add the argument `--fp16` in the GPT_ARGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=4      \n",
    "GLOBAL_BATCH_SIZE=16    \n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_increase_GBS_fp16\"        \n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            --fp16 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "            \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name fp16 \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh](./code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 13\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                13  slurmpar dli_2nod    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# submit the 2 nodes jobs\n",
    "! sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the Megatron GPT3 pretraining, we can check the generated logs during execution.\n",
    "\n",
    "Let's first look at the generated [logs](./megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16.txt) and check the world size of our executed run. We should see this:\n",
    "\n",
    "```\n",
    "    using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
    "    using torch.float16 for parameters ...\n",
    "```\n",
    "\n",
    "Notice that we have the message `torch.float16 for parameters` as we are running in fp16 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, let's check the training performance and discuss that with the instructor. Please notice that if you are running Megatron-LM for the first time in the class, the code will need about 6 minutes to be compiled. Until there, you will not be able to see any GPU activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration       10/     100 | consumed samples:          160 | elapsed time per iteration (ms): 323.4 | learning rate: 0.000E+00 | global batch size:    16 | loss scale: 8388608.0 | number of skipped iterations:  10 | number of nan iterations:   0 |\n",
      " iteration       20/     100 | consumed samples:          320 | elapsed time per iteration (ms): 178.3 | learning rate: 3.000E-05 | global batch size:    16 | lm loss: 1.070161E+01 | loss scale: 131072.0 | grad norm: 4.099 | number of skipped iterations:   6 | number of nan iterations:   0 |\n",
      " iteration       30/     100 | consumed samples:          480 | elapsed time per iteration (ms): 181.4 | learning rate: 5.999E-05 | global batch size:    16 | lm loss: 1.022598E+01 | loss scale: 65536.0 | grad norm: 2.543 | number of skipped iterations:   1 | number of nan iterations:   0 |\n"
     ]
    }
   ],
   "source": [
    "! grep elapsed /dli/megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Profiler with Tensorboard\n",
    "\n",
    "The profiling is available on the Tensorboard link at the `pytorch_profiler` tab for the previous run. In case you already closed the Tensorboard page, you can re-generate the link by executing the next cell. Click the link to open Tensorboard and then, go to the `PYTORCH_PROFILER` tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `run_fp16_gpu0` results, the GPU utilization is reduced. From the GPU Kernel view, we can see that when enabling Mixed precision, 23.6% of GPU operations are accelerated with TensorCores.\n",
    "\n",
    "<img src=\"images/profiling5_AMP.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "### How about the memory?\n",
    "We can see that the memory consumption is increased with a peak of 12G (~10G in the baseline). This is because some model weights are stored both in FP32 and FP16 while the activations and gradients are stored in FP16. Thus, the memory usage from the weights is increased while less memory is used by the activations and gradients at the forward and backward passes. When zooming into the new jumps, it shows an additional Pytorch copy operations *_to_copy*.\n",
    "\n",
    "\n",
    "<img src=\"images/profiling_FP16_memory.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "! rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.2 Activation Checkpointing \n",
    "\n",
    "\n",
    "<img src=\"images/activation_checkpoiting.png\" width=\"700\" align=\"center\"/>\n",
    "\n",
    "Activation Checkpointing is another technique allowing us to save memory during the training with the cost of additional re-compute. In the vanilla forward and backward pass, all feature maps are computed during the forward pass and stored for the backward step. In the activation checkpointing strategy, only some intermediate results are stored (called checkpoints) during the forward pass. Those checkpoints will be used at the backwards pass to recompute further feature maps when needed. There are several implementations of checkpointing, including manual checkpoints provided by the user or automatic selection.\n",
    "\n",
    "Megatron-LM supports two activation checkpointing methods: uniform and block.\n",
    "\n",
    "- Uniform: Uniformly divides the layers into groups and stores the input activations of each group in memory. \n",
    "- Block: Checkpoints the input activations of a set number of individual Transformer layers per pipeline stage.\n",
    "\n",
    "To run Megatron-LM pretraining with Activation Checkpointing, simply add the argument `--activations-checkpoint-method` to uniform or block in the GPT_ARGS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=4      \n",
    "GLOBAL_BATCH_SIZE=16    \n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing\"        \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            --fp16 \\\n",
    "            --activations-checkpoint-method uniform \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "            \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name  fp16_activation_checkpointing \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing.sh](/dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpoiting.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 14\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                14  slurmpar dli_2nod    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "! sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the Megatron GPT3 pretraining, we can check the generated [logs](./megatron/logs/log_2Nodes4GPUS_increaseBS_fp16_activation_checkpointing.txt) and discuss them with the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep elapsed /dli/megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the memory?\n",
    "\n",
    "The profiling is available on the Tensorboard link at the `pytorch_profiler` tab. When using activation checkpointing, we can see at `run_fp16_activation_checkpointing_GPU0` that the memory consumption is reduced considerably to a peak of ~3G. This is since some activations are not stored and recomputed when necessary.\n",
    "By zooming into the graph, we can trace the Pytorch CheckpointFunctions. \n",
    "\n",
    "<img src=\"images/profiling_FP16_checkpoiting_memory.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "! rm -rf /dli/megatron/checkpoints/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.3 Gradient Accumulation \n",
    "\n",
    "\n",
    "Another way of increasing the batch size is to use gradient accumulation. Instead of splitting the data across workers as in Distributed Data Parallel, in the Gradient Accumulation technique, the same worker processes several batches and accumulates the gradients before updating the model’s parameters.\n",
    "\n",
    "[NVIDIA APEX Library](https://github.com/NVIDIA/apex#quick-start) provides an optimized implementation when using Gradient Accumulation with Automatic Mixed precision. This implementation removes unnecessary double precision copies of the gradient by accumulating it in low precision first before going back to double precision. Megatron-LM library uses the APEX implementation.  \n",
    "\n",
    "\n",
    "To run Megatron-LM pretraining with Gradient Accumulation, simply increase the global batch size while maintaining the micro batch size the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing_gradient_accumulation.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing_gradient_accumulation.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=1       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# SLURM args\n",
    "MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "MASTER_PORT=6000\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=4      \n",
    "GLOBAL_BATCH_SIZE=64    \n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "VOCAB_SIZE=50257\n",
    "\n",
    "# Data Paths\n",
    "DATA_OUTPUT_PATH=/dli/megatron/checkpoints/test\n",
    "CHECKPOINT_PATH=/dli/megatron/checkpoints\n",
    "TENSORBOARD_PATH=/dli/megatron/tensorboard\n",
    "LOGS_PATH=/dli/megatron/logs\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=/dli/data/GPT-2_assets/my-gpt2_text_document\n",
    "\n",
    "NAME=\"log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing_gradient_accumulation\"        \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            --optimizer adam \\\n",
    "            --adam-beta1 0.9 \\\n",
    "            --adam-beta2 0.95 \\\n",
    "            --adam-eps 1e-8 \\\n",
    "            --lr 6e-5 \\\n",
    "            --min-lr 6e-6 \\\n",
    "            --lr-decay-style cosine \\\n",
    "            --lr-decay-iters 800 \\\n",
    "            --lr-warmup-fraction .01 \\\n",
    "            --clip-grad 1.0 \\\n",
    "            --weight-decay 1e-1 \\\n",
    "            --exit-duration-in-mins 1190 \\\n",
    "              \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            --num-layers $NLAYERS \\\n",
    "            --hidden-size $NHIDDEN \\\n",
    "            --num-attention-heads $NHEADS \\\n",
    "            --seq-length $SEQ_LEN \\\n",
    "            --max-position-embeddings $SEQ_LEN \\\n",
    "            --micro-batch-size $MICRO_BATCH_SIZE \\\n",
    "            --global-batch-size $GLOBAL_BATCH_SIZE \\\n",
    "            --train-iters 100 \\\n",
    "            --vocab-file $VOCAB_FILE \\\n",
    "            --merge-file $MERGE_FILE \\\n",
    "            --init-method-std 0.006 \\\n",
    "            --fp16 \\\n",
    "            --activations-checkpoint-method uniform \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "            \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            --log-interval 10 \\\n",
    "            --save-interval 300 \\\n",
    "            --eval-interval 1000 \\\n",
    "            --eval-iters 10 \\\n",
    "            --tensorboard-dir $TENSORBOARD_PATH \\\n",
    "            --tensorboard-queue-size 1 \\\n",
    "            --log-timers-to-tensorboard \\\n",
    "            --log-batch-size-to-tensorboard \\\n",
    "            --log-validation-ppl-to-tensorboard \\\n",
    "            --profile-execution True \\\n",
    "            --profile-name fp16_activation_checkpointing_gradient_accumulation \\\n",
    "            \"\n",
    "\n",
    "export LAUNCHER=\"python -u -m torch.distributed.launch \\\n",
    "             --nproc_per_node $GPUS_PER_NODE \\\n",
    "             --nnodes $NNODES \\\n",
    "             --master_addr $MASTER_ADDR \\\n",
    "             --master_port $MASTER_PORT \\\n",
    "             \"\n",
    "\n",
    "export CMD=\" \\\n",
    "             /dli/megatron/Megatron-LM/pretrain_gpt.py \\\n",
    "             --tensor-model-parallel-size $TP_SIZE \\\n",
    "             --pipeline-model-parallel-size $PP_SIZE \\\n",
    "             $GPT_ARGS \\\n",
    "             $OUTPUT_ARGS \\\n",
    "             --save $CHECKPOINT_PATH \\\n",
    "             --data-path $DATA_PATH \\\n",
    "             --data-impl mmap \\\n",
    "             --split 949,50,1 \\\n",
    "             --distributed-backend nccl \\\n",
    "           \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO  $LAUNCHER --node_rank $SLURM_PROCID $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing_gradient_accumulation.sh](/dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing_gradient_accumulation.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 15\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                15  slurmpar dli_2nod    admin PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_BS_fp16_activation_checkpointing_gradient_accumulation.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the Megatron GPT3 pretraining, we can check the generated [logs](./megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing_gradient_accumulation.txt) and discuss them with the instructor.\n",
    "\n",
    "Let's compare the number of micro-batches per GPU when enabling or disabling Gradient Accumulation. To do so, let’s compare to the previous runs without Gradient Accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without gradient accumulation:\n",
      "setting number of micro-batches to constant 1\n",
      "With 4 gradient accumulation:\n"
     ]
    }
   ],
   "source": [
    "print(\"Without gradient accumulation:\")\n",
    "!grep constant /dli/megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing.txt\n",
    "\n",
    "print(\"With 4 gradient accumulation:\")\n",
    "!grep constant /dli/megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing_gradient_accumulation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep elapsed /dli/megatron/logs/log_2Nodes4GPUS_increase_GBS_fp16_activation_checkpointing_gradient_accumulation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the memory?\n",
    "\n",
    "The profiling is available in Tensorboard link at the `pytorch_profiler` tab under the run `run_fp16_activation_checkpointing_gradient_accumulation_gpu0`. \n",
    "\n",
    "We can see the 4 gradient accumulations stages per step on the memory tracing. \n",
    "\n",
    "<img src=\"images/profiling_FP16_checkpoiting_gradient_acc_memory.png\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "! rm -rf /dli/megatron/checkpoints/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.4 The Training Performance\n",
    "\n",
    "In order to train large Neural Networks in a reasonable time, scaling the infrastructure is unavoidable. \n",
    "Let's first compute the number of parameters of our transformer models and estimate its training according to the hardware infrastructure and the experimentally observed training throughput.\n",
    "\n",
    "\n",
    "## 4.4.1 Compute the Number of Parameters of Transformers Model\n",
    "\n",
    "The number of parameters for a Transformers model is computed as:\n",
    "$P = 12 l h^2 (1 + \\frac{13}{12h} + \\frac{V+s}{12lh})$ where:\n",
    "- $l$ = Number of Layers\n",
    "- $h$ = Hidden Size\n",
    "- $V$ = Vocabulary Size\n",
    "- $s$ = Sequence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters of the Transformers model\n",
    "def calculate_number_parameters(l,h,s,V):\n",
    "    # Compute the number of parameters of the model\n",
    "    P=12*l*h*h *(1+ (13/(12*h)) + ((V+s)/(12*l*h)))\n",
    "    print(\"The number of parameters for the GPT architecture is: {} \\n\".format(int(P)))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's compute the number of parameters of the transformer model having 40 layers, a hidden size of 6144, vocabulary size of 50257 and sequence length of 1024. This model should be approximately 18 billion parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters for the GPT architecture is: 18437806080 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model architecture parameters\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "P=calculate_number_parameters(l,h,s,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Exercise: Compute the Number of Parameters For Our Model\n",
    "\n",
    "Compute the number of parameters of the model we have been experimenting with in previous notebooks.\n",
    "Have a look at the model architecture arguments in any Megaton-LM GPT pretraining scripts from previous notebooks. \n",
    "\n",
    "If you get stuck, you can look at the [solution](solutions/ex4.1.2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters for the GPT architecture is: 124438272 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124438272.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our model architecture parameters\n",
    "l=12\n",
    "h=768\n",
    "s=1024\n",
    "V=50257\n",
    "    \n",
    "calculate_number_parameters(l,h,s,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.3 Compute the Theoretical Peak FLOP per second per GPU\n",
    "\n",
    "As detailed in the paper [Scaling Language Model Training to a Trillion Parameters Using Megatron](https://arxiv.org/pdf/2104.04473.pdf), the majority of floating-point operations in the model are performed in the matrix multiplications (GEMMs). If we consider only these GEMMs operations, the number of FLOPs per iteration is $F = 96 B s l h^2 (1 + \\frac{s}{6h} + \\frac{V}{16lh})$ where $B$ is the batch size. \n",
    "\n",
    "And, in case we have an estimate of the time spent per iteration `Time_per_iteration_second`, it is possible then to compute the theoretical peak FLOP per second and per GPUs and estimate the GPU usage by comparing. \n",
    "\n",
    "The following table shows the training performance of several GPT model sizes (from 1.7B to 1 trillion) pretrained using Megatron-LM library on a SuperPOD cluster with A100 GPUs.\n",
    "<img src=\"https://github.com/NVIDIA/Megatron-LM/blob/main/images/cases_april2021.png?raw=true\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical peak FLOP per second per GPU - with activation checkpointing (2 forwards and 1 backward)\n",
    "def calculate_Theoretical_peak_FLOP_s_GPU(B,s, l,h,number_GPUs,Time_per_iteration_second):\n",
    "    # The number of FLOPs per iteration\n",
    "    F = 96*B*s* l*h*h *(1 + s/ (6*h) + V/(16*l*h))/1e+12\n",
    "    \n",
    "    #Theoretical peak FLOP per second per GPU\n",
    "    PF= (F/Time_per_iteration_second/number_GPUs)\n",
    "    print(\"Theoretical peak FLOP/s/GPU: {}\\n\".format(PF))\n",
    "    \n",
    "    # Percentage of theoretical peak FLOP/s on a A100 FP16 (change according the hardware)\n",
    "    GPU_usage= PF/ 312 *100\n",
    "    print(\"Percentage of theoretical peak FLOP/s: {}%\".format(GPU_usage))\n",
    "    \n",
    "    return PF, GPU_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of theoretical peak FLOP/s in the previous function is based on **A100** hardware capabilities in **FP16/BF16 which is 312**. This needs to be updated according to the corresponding Tensor Core GPU performance specs. To learn more about the [Ampere architecture specifications](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/).\n",
    "\n",
    "\n",
    "If we consider our previous 18B parameters models pretrained on 16 GPUs with a global batch size of 512, they have a time per iteration of 32.09s. \n",
    "Let's compute the theoretical peak FLOP per second per GPU and the percentage GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical peak FLOP/s/GPU: 157.7296879710454\n",
      "\n",
      "Percentage of theoretical peak FLOP/s: 50.55438717020686%\n"
     ]
    }
   ],
   "source": [
    "global_batch_size=512\n",
    "number_GPUs=16\n",
    "Time_per_iteration_second=32.09\n",
    "\n",
    "# Considering the 18B parameters model\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "PF,GPU_usage=calculate_Theoretical_peak_FLOP_s_GPU(global_batch_size,s, l,h,number_GPUs,Time_per_iteration_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.4 Estimate the Training Duration / Epoch\n",
    "\n",
    "It is possible to estimate training duration per epoch according to the model, dataset, and hardware size. Training time (in seconds) is approximated with this equation $\\approx \\frac{8*T*P}{n * PF}$ where: \n",
    "- $T$ = Number of tokens in the dataset\n",
    "- $P$ = Numbers of parameters \n",
    "- $n$ = Number of GPUs\n",
    "- $PF$ = Achieved teraFLOP/s per GPU\n",
    "\n",
    "More details are described in the paper [Scaling Language Model Training to a Trillion Parameters Using Megatron](https://arxiv.org/pdf/2104.04473.pdf).\n",
    "\n",
    "Let's execute the 2 following cells to estimate the training duration for the 18B parameters models trained on a dataset of $T$=300 billion tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# Estimate the training time\n",
    "def estimate_days_needed(T , P , N ,PF):  \n",
    "    compute_sec=8*T*P/(N*PF*10e12)\n",
    "    # Convert compute seconds to days\n",
    "    to_days=round(compute_sec/(3600*24))\n",
    "    print(\"This language model will need {} days per epoch.\".format(colored(str(to_days),'blue', attrs=['bold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This language model will need \u001b[1m\u001b[34m203\u001b[0m days per epoch.\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens in the dataset\n",
    "T=300*10e09\n",
    "\n",
    "estimate_days_needed(T,P,number_GPUs,PF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 203 days, which is almost **7 months** required to train the 18B model on 16 GPUs (2 nodes) with a dataset of 300B tokens! \n",
    "\n",
    "In this case, scaling the number of nodes is unavoidable in order to train the model in a reasonable amount of time. \n",
    "\n",
    "For instance, consider a GPT-3 model with $P$=175 billion parameters trained on a dataset of $T$=300 billion tokens on $n$=1024 A100 GPUs. Using a batch size of 1536, we achieve $F$=140 teraFLOP/s per GPU. Thus, the time required to train this model is **34 days**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, we will experiment with other techniques used for training large-scale neural networks and demonstrate their usage for Computer Vision. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
