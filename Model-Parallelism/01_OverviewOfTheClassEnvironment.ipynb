{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Overview of the Class Environment\n",
    "\n",
    "This notebook will introduce the basic knowledge of using AI clusters. You will have an overview of the Class Environment configured as an AI compute cluster. In addition, you will experiment with basic commands of the [SLURM cluster management](https://slurm.schedmd.com/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the hardware configuration available for the class\n",
    "* Understand the basics commands for jobs submissions with SLURM\n",
    "* Run simple test scripts allocating different GPU resources\n",
    "* Connect interactively to a compute node and observe available resources\n",
    "\n",
    "**[1.1 The Hardware Configuration Overview](#1.1-The-Hardware-Configuration-Overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.1 Check The Available CPUs](#1.1.1-Check-The-Available-CPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.2 Check the Available GPUs](#1.1.2-Check-The-Available-GPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.3 Check The Interconnect Topology](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.4 Bandwidth & Connectivity Tests](#1.1.4-Bandwidth-and-Connectivity-Tests)<br>\n",
    "**[1.2 Basic SLURM Commands](#1.2-Basic-SLURM-Commands)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.1 Check the SLURM Configuration](#1.2.1-Check-the-SLURM-Configuration)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.2 Submit Jobs Using SRUN Command](#1.2.2-Submit-jobs-using-SRUN-Command)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.3 Submit Jobs Using SBATCH Command](#1.2.3-Submit-jobs-using-SBATCH-Command])<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.4 Exercise: Submit Jobs Using SBATCH Command Requesting More Resources](#1.2.4-Exercise-Submit-jobs-using-SBATCH-Command])<br>\n",
    "**[1.3 Run Interactive Sessions](#1.3-Run-Interactive-Sessions)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.1 The Hardware Configuration Overview\n",
    "\n",
    "\n",
    "A modern AI cluster is a type of infrastructure designed for optimal Deep Learning model development. NVIDIA has designed DGXs servers as a full-stack solution for scalable AI development. Click the link to learn more about [DGX systems](https://www.nvidia.com/en-gb/data-center/dgx-systems/).\n",
    "\n",
    "In this lab, in terms of GPU and networking hardware resources, each class environment is configured to access about half the resources of a DGX-1 server system (4 V100 GPUs, 4 NVlinks per GPU).\n",
    "\n",
    "<img  src=\"images/nvlink_v2.png\" width=\"600\"/>\n",
    "\n",
    "The hardware for this class has already been configured as a GPU cluster unit for Deep Learning. The cluster is organized as compute units (nodes) that can be allocated using a Cluster Manager (example SLURM). Among the hardware components, the cluster includes CPUs (Central Processing Units), GPUs (Graphics Processing Units), storage and networking.\n",
    "\n",
    "Let's look at the GPUs, CPUs and network design available in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Check The Available CPUs \n",
    "\n",
    "We can check the CPU information of the system using the `lscpu` command. \n",
    "\n",
    "This example of outputs shows that there are 16 CPU cores of the `x86_64` from Intel.\n",
    "```\n",
    "Architecture:                    x86_64\n",
    "Core(s) per socket:              16\n",
    "Model name:                      Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
    "```\n",
    "For a complete description of the CPU processor architecture, check the `/proc/cpuinfo` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                    x86_64\n",
      "CPU op-mode(s):                  32-bit, 64-bit\n",
      "Byte Order:                      Little Endian\n",
      "Address sizes:                   48 bits physical, 48 bits virtual\n",
      "CPU(s):                          96\n",
      "On-line CPU(s) list:             0-95\n",
      "Thread(s) per core:              1\n",
      "Core(s) per socket:              48\n",
      "Socket(s):                       2\n",
      "NUMA node(s):                    4\n",
      "Vendor ID:                       AuthenticAMD\n",
      "CPU family:                      25\n",
      "Model:                           1\n",
      "Model name:                      AMD EPYC 7V13 64-Core Processor\n",
      "Stepping:                        1\n",
      "CPU MHz:                         2464.505\n",
      "BogoMIPS:                        4890.88\n",
      "Hypervisor vendor:               Microsoft\n",
      "Virtualization type:             full\n",
      "L1d cache:                       3 MiB\n",
      "L1i cache:                       3 MiB\n",
      "L2 cache:                        48 MiB\n",
      "L3 cache:                        384 MiB\n",
      "NUMA node0 CPU(s):               0-23\n",
      "NUMA node1 CPU(s):               24-47\n",
      "NUMA node2 CPU(s):               48-71\n",
      "NUMA node3 CPU(s):               72-95\n",
      "Vulnerability Itlb multihit:     Not affected\n",
      "Vulnerability L1tf:              Not affected\n",
      "Vulnerability Mds:               Not affected\n",
      "Vulnerability Meltdown:          Not affected\n",
      "Vulnerability Spec store bypass: Vulnerable\n",
      "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user\n",
      "                                  pointer sanitization\n",
      "Vulnerability Spectre v2:        Mitigation; Full AMD retpoline, STIBP disabled,\n",
      "                                  RSB filling\n",
      "Vulnerability Srbds:             Not affected\n",
      "Vulnerability Tsx async abort:   Not affected\n",
      "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n",
      "                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n",
      "                                 se2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtsc\n",
      "                                 p lm constant_tsc rep_good nopl tsc_reliable no\n",
      "                                 nstop_tsc cpuid extd_apicid aperfmperf pni pclm\n",
      "                                 ulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe p\n",
      "                                 opcnt aes xsave avx f16c rdrand hypervisor lahf\n",
      "                                 _lm cmp_legacy cr8_legacy abm sse4a misalignsse\n",
      "                                  3dnowprefetch osvw topoext perfctr_core invpci\n",
      "                                 d_single vmmcall fsgsbase bmi1 avx2 smep bmi2 i\n",
      "                                 nvpcid rdseed adx smap clflushopt clwb sha_ni x\n",
      "                                 saveopt xsavec xgetbv1 xsaves clzero xsaveerptr\n",
      "                                  arat umip vaes vpclmulqdq rdpid\n"
     ]
    }
   ],
   "source": [
    "# Display information CPUs\n",
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cores\t: 48\n"
     ]
    }
   ],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Check The Available  GPUs \n",
    "\n",
    "The NVIDIA System Management Interface `nvidia-smi` is a command for monitoring NVIDIA GPU devices. Several key details are listed such as the CUDA and  GPU driver versions, the number and type of GPUs available, the GPU memory each, running GPU process, etc.\n",
    "\n",
    "In the following example, `nvidia-smi` command shows that there are 4 Tesla V100-SXM2 GPUs (ID 0-3), each with 16GB of memory. \n",
    "\n",
    "<img  src=\"images/nvidia_smi.png\" width=\"600\"/>\n",
    "\n",
    "For more details, refer to the [nvidia-smi documentation](https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 16:58:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Graphics Device     On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    53W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Graphics Device     On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Graphics Device     On   | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    54W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Graphics Device     On   | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    52W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Display information about GPUs\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Check The Available Interconnect Topology \n",
    "\n",
    "\n",
    "\n",
    "<img  align=\"right\" src=\"images/nvlink_nvidia.png\" width=\"420\"/>\n",
    "\n",
    "The multi-GPU system configuration needs a fast and scalable interconnect. [NVIDIA NVLink technology](https://www.nvidia.com/en-us/data-center/nvlink/) is a direct GPU-to-GPU interconnect providing high bandwidth and improving scalability for multi-GPU systems.\n",
    "\n",
    "To check the available interconnect topology, we can use `nvidia-smi topo --matrix` command. In this class, we should get 4 NVLinks per GPU device. \n",
    "\n",
    "```\n",
    "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      NV1     NV1     NV2     0-31            N/A\n",
    "GPU1    NV1      X      NV2     NV1     0-31            N/A\n",
    "GPU2    NV1     NV2      X      NV2     0-31            N/A\n",
    "GPU3    NV2     NV1     NV2      X      0-31            N/A\n",
    "\n",
    "Where X= Self and NV# = Connection traversing a bonded set of # NVLinks\n",
    "```\n",
    "\n",
    "In this environment, notice only 1 link between GPU0 and GPU1, GPU2 while 2 links are shown between GPU0 and GPU3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\u001b[0m\n",
      "GPU0\t X \tNV12\tSYS\tSYS\t0-23\t0\n",
      "GPU1\tNV12\t X \tSYS\tSYS\t24-47\t1\n",
      "GPU2\tSYS\tSYS\t X \tNV12\t48-71\t2\n",
      "GPU3\tSYS\tSYS\tNV12\t X \t72-95\t3\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "# Check Interconnect Topology \n",
    "! nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to check the NVLink status and bandwidth using `nvidia-smi nvlink --status` command. You should see similar outputs per device.\n",
    "```\n",
    "GPU 0: Tesla V100-SXM2-16GB \n",
    "\t Link 0: 25.781 GB/s\n",
    "\t Link 1: 25.781 GB/s\n",
    "\t Link 2: 25.781 GB/s\n",
    "\t Link 3: 25.781 GB/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Graphics Device (UUID: GPU-92bb16cd-3f7b-fff6-85ba-d59f3ee01fec)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 1: Graphics Device (UUID: GPU-000d0a4b-3c11-527c-5b9e-e9000609784a)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 2: Graphics Device (UUID: GPU-4aa50531-07c0-fe5d-efca-a1fb8357d663)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 3: Graphics Device (UUID: GPU-659119cb-c367-4cbb-a547-d02fb3aba084)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n"
     ]
    }
   ],
   "source": [
    "# Check nvlink status\n",
    "! nvidia-smi nvlink --status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Bandwidth & Connectivity Tests\n",
    "\n",
    "\n",
    "NVIDIA provides an application **p2pBandwidthLatencyTest** that demonstrates CUDA Peer-To-Peer (P2P) data transfers between pairs of GPUs by computing bandwidth and latency while enabling and disabling NVLinks. This tool is part of the code samples for CUDA Developers [cuda-samples](https://github.com/NVIDIA/cuda-samples.git). \n",
    "\n",
    "Example outputs are shown below. Notice the Device to Device (D\\D) bandwidth differences when enabling and disabling NVLinks (P2P).\n",
    "\n",
    "```\n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 780.27  48.49  48.49  96.89 \n",
    "     1  48.49 777.94  96.91  48.49 \n",
    "     2  48.49  96.85 779.30  96.90 \n",
    "     3  96.89  48.49  96.89 779.11 \n",
    "\n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 777.94   9.56  14.41  14.48 \n",
    "     1   9.49 780.47  14.19  14.16 \n",
    "     2  14.39  14.18 779.89   9.51 \n",
    "     3  14.44  14.24   9.65 780.08\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
      "Device: 0, Graphics Device, pciBusID: 0, pciDeviceID: 0, pciDomainID:1\n",
      "Device: 1, Graphics Device, pciBusID: 0, pciDeviceID: 0, pciDomainID:2\n",
      "Device: 2, Graphics Device, pciBusID: 0, pciDeviceID: 0, pciDomainID:3\n",
      "Device: 3, Graphics Device, pciBusID: 0, pciDeviceID: 0, pciDomainID:4\n",
      "Device=0 CAN Access Peer Device=1\n",
      "Device=0 CANNOT Access Peer Device=2\n",
      "Device=0 CANNOT Access Peer Device=3\n",
      "Device=1 CAN Access Peer Device=0\n",
      "Device=1 CANNOT Access Peer Device=2\n",
      "Device=1 CANNOT Access Peer Device=3\n",
      "Device=2 CANNOT Access Peer Device=0\n",
      "Device=2 CANNOT Access Peer Device=1\n",
      "Device=2 CAN Access Peer Device=3\n",
      "Device=3 CANNOT Access Peer Device=0\n",
      "Device=3 CANNOT Access Peer Device=1\n",
      "Device=3 CAN Access Peer Device=2\n",
      "\n",
      "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
      "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
      "\n",
      "P2P Connectivity Matrix\n",
      "     D\\D     0     1     2     3\n",
      "     0\t     1     1     0     0\n",
      "     1\t     1     1     0     0\n",
      "     2\t     0     0     1     1\n",
      "     3\t     0     0     1     1\n",
      "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1503.85  21.64  21.56  21.46 \n",
      "     1  21.65 1502.40  21.57  21.48 \n",
      "     2  17.96  18.11 1503.85  16.97 \n",
      "     3  17.72  17.84  16.76 1511.12 \n",
      "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1505.30 275.42  21.59  21.52 \n",
      "     1 274.97 1519.94  21.56  21.53 \n",
      "     2  17.95  18.11 1508.20 275.43 \n",
      "     3  17.72  17.84 274.98 1516.99 \n",
      "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1529.61  29.09  20.34  20.17 \n",
      "     1  28.23 1528.12  20.42  20.25 \n",
      "     2  20.85  20.92 1528.86  18.41 \n",
      "     3  20.60  20.68  18.41 1530.36 \n",
      "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1529.61 517.65  20.35  20.16 \n",
      "     1 517.06 1528.86  20.41  20.26 \n",
      "     2  20.79  20.91 1530.36 518.25 \n",
      "     3  20.59  20.70 516.71 1529.61 \n",
      "P2P=Disabled Latency Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.38  12.46  16.83  18.23 \n",
      "     1  12.48   2.47  15.91  16.17 \n",
      "     2  11.71  12.29   2.63  11.53 \n",
      "     3  18.09  11.77  11.71   2.32 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.47   6.52   7.27   6.78 \n",
      "     1   5.90   2.36   7.25   6.72 \n",
      "     2   6.47   6.33   2.71   7.91 \n",
      "     3   7.10   6.48   7.36   2.36 \n",
      "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.37   2.24  18.65  17.48 \n",
      "     1   2.32   2.45  15.78  18.97 \n",
      "     2  20.25  11.67   2.63   2.32 \n",
      "     3  16.07  20.27   2.33   2.27 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.45   1.61   7.36   6.92 \n",
      "     1   1.70   2.13   6.70   6.83 \n",
      "     2   6.51   7.00   2.64   1.92 \n",
      "     3   6.76   6.54   1.97   2.46 \n",
      "\n",
      "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Tests on GPU pairs using P2P and without P2P \n",
    "#`git clone --depth 1 --branch v11.2 https://github.com/NVIDIA/cuda-samples.git`\n",
    "! /dli/cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Basic SLURM Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how GPUs can communicate with each other over NVLink, let's go over how the hardware resources can be organized into compute nodes. These nodes can be managed by Cluster Manager such as [*Slurm Workload Manager*](https://slurm.schedmd.com/), an open source cluster management and job scheduler system for large and small Linux clusters. \n",
    "\n",
    "\n",
    "For this lab, we have configured a SLURM manager where the 4 available GPUs are partitioned into 2 nodes: **slurmnode1** \n",
    "and **slurmnode2**, each with 2 GPUs. \n",
    "\n",
    "Next, let's see some basic SLURM commands. More SLURM commands can be found in the [SLURM official documentation](https://slurm.schedmd.com/).\n",
    "\n",
    "<img src=\"images/cluster_overview.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Check the SLURM Configuration\n",
    "\n",
    "We can check the available resources in the SLURM cluster by running `sinfo`. The output will show that there are 2 nodes in the cluster **slurmnode1** and **slurmnode2**. Both nodes are currently idle.\n",
    "\n",
    "![title](images/slurm_config.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "slurmpar*    up   infinite      2   idle slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Check available resources in the cluster\n",
    "! sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2.2 Submit Jobs Using `srun` Command\n",
    "\n",
    "The `srun` command allows to running parallel jobs. \n",
    "\n",
    "The argument **-N** (or *--nodes*) can be used to specify the nodes allocated to a job. It is also possible to allocate a subset of GPUs available within a node by specifying the argument **-G (or --gpus)**.\n",
    "\n",
    "Check out the [SLURM official documentation](https://slurm.schedmd.com/) for more arguments.\n",
    "\n",
    "To test running parallel jobs, let's submit a job that requests 1 node (2 GPUs) and run a simple command on it: `nvidia-smi`. We should see the output of 2 GPUs available in the allocated node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 16:59:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Graphics Device     On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    53W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Graphics Device     On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    54W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# run nvidia-smi slurm job with 1 node allocation\n",
    "! srun -N 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's now allocate 2 nodes and run again `nvidia-smi` command.\n",
    "\n",
    "We should see the results of both nodes showing the available GPU devices. Notce that the stdout might be scrumbled due to the asynchronous and parallel execution of `nvidia-smi` command in the two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 23 17:00:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "Thu Mar 23 17:00:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Graphics Device     On   | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    54W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   0  Graphics Device     On   | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    53W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Graphics Device     On   | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    52W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|   1  Graphics Device     On   | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    53W / 300W |      0MiB / 81251MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# run nvidia-smi slurm job with 2 node allocation.\n",
    "! srun -N 2 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Submit Jobs Using `sbatch` Command \n",
    "\n",
    "In the previous examples, we allocated resources to run one single command. For more complex jobs, the `sbatch` command allows submitting batch scripts to SLURM by specifying the resources and all environment variables required for executing the job. `sbatch` will transfer the execution to the SLURM Manager after automatically populating the arguments.\n",
    "\n",
    "In the batch script below, `#SBATCH ...` is used to specify resources and other options relating to the job to be executed:\n",
    "\n",
    "```\n",
    "        #!/bin/bash\n",
    "        #SBATCH -N 1                               # Node count to be allocated for the job\n",
    "        #SBATCH --job-name=dli_firstSlurmJob       # Job name\n",
    "        #SBATCH -o /dli/megatron/logs/%j.out       # Outputs log file \n",
    "        #SBATCH -e /dli/megatron/logs/%j.err       # Errors log file\n",
    "\n",
    "        srun -l my_script.sh                       # my SLURM script \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we submit the `sbatch` batch script, let's first prepare a job that will be executed: a short batch script that will sleep for 2 seconds before running the `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x /dli/code/test.sh\n",
    "# Check the batch script \n",
    "!cat /dli/code/test.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this batch script job, let's create an `sbatch` script that initiates the resources to be allocated and submits the test.sh job.\n",
    "\n",
    "The following cell will edit the `test_sbatch.sbatch` script allocating 1 node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /dli/code/test_sbatch.sbatch\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -N 1\n",
    "#SBATCH --job-name=dli_firstSlurmJob\n",
    "#SBATCH -o /dli/megatron/logs/%j.out\n",
    "#SBATCH -e /dli/megatron/logs/%j.err\n",
    "\n",
    "srun -l /dli/code/test.sh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the sbatch script \n",
    "! cat /dli/code/test_sbatch.sbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit the `sbatch` job and check the SLURM scheduler. The batch script will be queued and executed when the requested resources are available.\n",
    "\n",
    "The `squeue` command shows the running or pending jobs. An output example is shown below: \n",
    "\n",
    "```\n",
    "Submitted batch job **\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "                **  slurmpar test_sba    admin  R       0:01      1 slurmnode1\n",
    "\n",
    "```\n",
    "\n",
    "It shows the SLURM Job ID, Job's name, the user ID, Job's Status (R=running), running duration and the allocated node name.\n",
    "\n",
    "The following cell submits the `sbatch` job, collects the `JOBID` variable (for querying later the logs) and checks the jobs in the SLURM scheduling queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the job\n",
    "! sbatch /dli/code/test_sbatch.sbatch\n",
    "\n",
    "# Get the JOBID variable\n",
    "JOBID=!squeue -u admin | grep dli | awk '{print $1}'\n",
    "slurm_job_output='/dli/megatron/logs/'+JOBID[0]+'.out'\n",
    "\n",
    "# check the jobs in the SLURM scheduling queue\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output log file for the executed job (**JOBID.out**) is automatically created to gather the outputs.\n",
    "\n",
    "In our case, we should see the results of `nvidia-smi` command that was executed in the `test.sh` script submitted with 1 node allocation. Let's have a look at execution logs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 3 seconds to let the job execute and get the populated logs \n",
    "! sleep 3\n",
    "\n",
    "# Check the execution logs \n",
    "! cat $slurm_job_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4  Exercise: Submit Jobs Using `sbatch` Command  Requesting More Resources\n",
    "\n",
    "\n",
    "Using what you have learned, submit the previous `test.sh` batch script with the `sbatch` command on **2 nodes** allocation.\n",
    "\n",
    "To do so, you will need to:\n",
    "1. Modify the `test_sbatch.sbatch` script to allocate 2 Nodes \n",
    "2. Submit the script again using `sbatch` command\n",
    "3. Check the execution logs \n",
    "\n",
    "\n",
    "If you get stuck, you can look at the [solution](solutions/ex1.2.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modify the `test_sbatch.sbatch` script to allocate 2 Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Submit the script again using `sbatch` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check the execution logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.3 Run Interactive Sessions \n",
    "\n",
    "Interactive sessions allow to connect directly to a worker node and interact with it through the terminal. \n",
    "\n",
    "The SLURM manager allows to allocate resources in interactive session using the `--pty` argument as follows: `srun -N 1 --pty /bin/bash`. \n",
    "The session is closed when you exit the node or you cancel the interactive session job using the command `scancel JOBID`.\n",
    "\n",
    "\n",
    "Since this is an interactive session, first, we need to launch a terminal window and submit a slurm job allocating resources in interactive mode. To do so, we will need to follow the 3 steps: \n",
    "1. Launch a terminal session\n",
    "2. Check the GPUs resources using the command `nvidia-smi` \n",
    "3. Run an interactive session requesting 1 node by executing `srun -N 1 --pty /bin/bash`\n",
    "4. Check the GPUs resources using the command `nvidia-smi` again \n",
    "\n",
    "Let's run our first interactive job requesting 1 node and check what GPU resources are at our disposal. \n",
    "\n",
    "![title](images/interactive_launch.png)\n",
    "\n",
    "Notice that while connected to the session, the host name as displayed in the command line changes from \"lab\" (login node name) to \"slurmnode1\" indicating that we are now successfully working on a remote worker node.\n",
    "\n",
    "Run the following cell to get a link to open a terminal session and the instructions to run an interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following this <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Check the GPUs resources: <font color=\"green\">nvidia-smi</font>\n",
    "   Step 3: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 4: Check the GPUs resources again: <font color=\"green\">nvidia-smi</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've made it through the first section of the course and are ready to begin training Deep Learning models on multiple GPUs. <br>\n",
    "\n",
    "Before moving on, we need to make sure that no jobs are still running or waiting on the SLURM queue. \n",
    "Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be running basic GPT language model training on different distribution configurations. Move on to [02_GPT_LM_pretrainings.ipynb](02_GPT_LM_pretrainings.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
